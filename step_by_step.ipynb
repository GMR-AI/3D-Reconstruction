{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import cv2\n",
    "import os"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Setup Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = cv2.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return images"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "# Load all images\n",
    "images = np.array(load_images_from_folder('dinos'))\n",
    "\n",
    "# First two images\n",
    "img1 = images[0]\n",
    "img2 = images[1]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Two Images SfM\n",
    "\n",
    "We're following the \"guide\" provided by this [GitHub's README by rohana96](https://github.com/rohana96/SfM), following an incremental SfM. However, we tried to make the different steps by ourselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Previous feature extraction and matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "def feature_extraction_set(images):\n",
    "    sift = cv2.SIFT_create()\n",
    "\n",
    "    kp, des = [], []\n",
    "    for im in images:\n",
    "        kp_tmp, des_tmp = sift.detectAndCompute(im, None) # This assumes the extraction method to be from the CV2 library\n",
    "        kp.append(kp_tmp)\n",
    "        des.append(des_tmp)\n",
    "    return kp, des # Can't turn them into a np array since their shape can be inhomogeneous"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "# Feature extraction\n",
    "kp, des = feature_extraction_set(images)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "def feature_matching_set(kp, des):\n",
    "    # Initialize FLANN matching\n",
    "    FLANN_INDEX_KDTREE = 0\n",
    "    index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n",
    "    search_params = dict(checks=50)\n",
    "    flann = cv2.FlannBasedMatcher(index_params, search_params)\n",
    "\n",
    "    matches = {} # Dict for easier access to each match\n",
    "    for i in range(len(kp)):\n",
    "        for j in range(i+1, len(kp)): # Only match each image with the rest, we don't need the full matrix\n",
    "            matches_tmp = flann.knnMatch(des[i], des[j], k=2)\n",
    "\n",
    "            # Lowe's ratio test\n",
    "            good_matches = [m for m, n in matches_tmp if m.distance < 0.7 * n.distance]\n",
    "\n",
    "            if len(good_matches) >= 4:\n",
    "                # RANSAC to find homography and get inlier's mask\n",
    "                pts1 = np.float32([kp[i][m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "                pts2 = np.float32([kp[j][m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)\n",
    "                _, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC, 5.0)\n",
    "\n",
    "                inliers = [good_matches[k] for k in range(len(good_matches)) if mask[k]==1]\n",
    "                matches[(i,j)] = inliers\n",
    "            else:\n",
    "                matches[(i,j)] = []\n",
    "    \n",
    "    return matches"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "# Feature matching\n",
    "matches = feature_matching_set(kp, des)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamental, Essential and Extrinsic decomposition\n",
    "\n",
    "For this process, we'll assume we know the intrinsic camera parameters in one way or the other. In this exact case, we extracted it from the dataset, which isn't custom.  \n",
    "Our goal is to get the extrinsic parameters. We can get them decomposing the Essential matrix into 4 possible outcomes.  \n",
    "And to get the Essential matrix we apply the next formula $E=K_1^T*F*K_2$. So, to begin with, we apply the [Eight Point algorithm](https://cseweb.ucsd.edu/classes/fa22/cse252A-a/lec10.pdf) to find the Fundamental matrix.  \n",
    "We don't directly calculate the Essential matrix because the images aren't calibrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "# K = np.loadtxt('intrinsic_matrix.txt', dtype=float)\n",
    "\n",
    "height, width = images.shape[1:3]\n",
    "K = np.array([  # for dino\n",
    "    [2360, 0, width / 2],\n",
    "    [0, 2360, height / 2],\n",
    "    [0, 0, 1]])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "def normalize(pts):\n",
    "    x_mean = np.mean(pts[:, 0])\n",
    "    y_mean = np.mean(pts[:, 1])\n",
    "    sigma = np.mean(np.sqrt((pts[:, 0] - x_mean) ** 2 + (pts[:, 1] - y_mean) ** 2))\n",
    "    M = np.sqrt(2) / sigma\n",
    "    T = np.array([\n",
    "        [M, 0, -M * x_mean],\n",
    "        [0, M, -M * y_mean],\n",
    "        [0, 0, 1]\n",
    "    ])\n",
    "    return T\n",
    "\n",
    "def eight_point_algorithm(pts1, pts2):\n",
    "\n",
    "    pts1_homo = np.vstack((pts1, np.ones(pts1.shape[1]))).T\n",
    "    pts2_homo = np.vstack((pts2, np.ones(pts2.shape[1]))).T\n",
    "\n",
    "    # Normalization\n",
    "    T = normalize(pts1_homo)\n",
    "    T_prime = normalize(pts2_homo)\n",
    "\n",
    "\n",
    "    pts1_homo = (T @ pts1_homo.T).T\n",
    "    pts2_homo = (T_prime @ pts2_homo.T).T\n",
    "\n",
    "    # x2.T*F*x1=0\n",
    "    # A*f=0, f is F flattened into a 1D array\n",
    "    \n",
    "\n",
    "    # Create A\n",
    "    A = np.zeros((pts1.shape[1], 9))\n",
    "    for i in range(pts1.shape[1]):\n",
    "        A[i] = np.array([\n",
    "            pts1_homo[i,0]*pts2_homo[i,0], pts1_homo[i,1]*pts2_homo[i,0], pts1_homo[i,2]*pts2_homo[i,0],\n",
    "            pts1_homo[i,0]*pts2_homo[i,1], pts1_homo[i,1]*pts2_homo[i,1], pts1_homo[i,2]*pts2_homo[i,1],\n",
    "            pts1_homo[i,0]*pts2_homo[i,2], pts1_homo[i,1]*pts2_homo[i,2], pts1_homo[i,2]*pts2_homo[i,2]\n",
    "            ])\n",
    "    \n",
    "    # Solve Af=0 using svd\n",
    "    U,S,Vt = np.linalg.svd(A)\n",
    "    F = Vt[-1,:].reshape((3,3))\n",
    "\n",
    "    # Enforce rank2 constraint\n",
    "    U,S,Vt = np.linalg.svd(F)\n",
    "    S[-1] = 0\n",
    "    F = U @ np.diag(S) @ Vt\n",
    "\n",
    "    F = T_prime.T @ F @ T\n",
    "    return F"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "# Fundamental matrix\n",
    "pts1 = np.transpose([kp[0][m.queryIdx].pt for m in matches[(0,1)]])\n",
    "pts2 = np.transpose([kp[1][m.trainIdx].pt for m in matches[(0,1)]])\n",
    "\n",
    "F = eight_point_algorithm(pts1, pts2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "def essential_from_fundamental(K1, F, K2):\n",
    "    return K1.T @ F @ K2"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "# Essential matrix\n",
    "E = essential_from_fundamental(K, F, K) # In this case, the same intrinsic values apply to all images"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "def pose_from_essential(E):\n",
    "    U,_,Vt = np.linalg.svd(E)\n",
    "    W = np.array([[0, -1, 0], [1, 0, 0], [0, 0, 1]])\n",
    "    \n",
    "    Rs = [U @ W @ Vt, U @ W.T @ Vt]\n",
    "    for i in range(len(Rs)):\n",
    "        if np.linalg.det(Rs[i]) < 0:\n",
    "            Rs[i] = Rs[i] * -1\n",
    "\n",
    "    # Array with all possible camera poses (extrinsics)\n",
    "    RTs = np.array([\n",
    "        np.hstack((Rs[0], U[:, 2, np.newaxis])),\n",
    "        np.hstack((Rs[0], -U[:, 2, np.newaxis])),\n",
    "        np.hstack((Rs[1], U[:, 2, np.newaxis])),\n",
    "        np.hstack((Rs[1], -U[:, 2, np.newaxis])),\n",
    "    ])\n",
    "\n",
    "    return RTs"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# Get camera extrinsics from Essential matrix\n",
    "RT2s = pose_from_essential(E)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# Define RT for camera 1 (center at world origin and matching orientation)\n",
    "RT1 = np.hstack((np.eye(3), np.zeros((3, 1))))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disambiguation\n",
    "\n",
    "The next step is to determine which is the right camera pose for image 2. For this, one method is to apply the cheirality condition.  \n",
    "This condition states that the sign of the Z element in the camera coordinate system indicates the location of the 3D point respect to the camera, so ideally the right configuration would be that which points are all Z positive. However, due to possible correspondece noise, the best camera configuration maximizes the number of points satisfying the cheirality condition. [Reference](https://inst.eecs.berkeley.edu/~ee290t/fa19/lectures/lecture10-3-decomposing-F-matrix-into-Rotation-and-Translation.pdf)  \n",
    "[Triangulation pdf](https://www.cs.cmu.edu/~16385/s17/Slides/11.4_Triangulation.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "def linear_triangulation(K1, RT1, K2, RT2, pts1, pts2):\n",
    "    # First, set all points to homogeneous\n",
    "    pts1_homo = np.vstack((pts1, np.ones(pts1.shape[1])))\n",
    "    pts2_homo = np.vstack((pts2, np.ones(pts2.shape[1])))\n",
    "\n",
    "    # Calculate every projection matrix\n",
    "    P1 = K1 @ RT1\n",
    "    P2 = K2 @ RT2\n",
    "\n",
    "    # Solve using svd\n",
    "    pts3d = np.zeros((3, pts1.shape[1]))\n",
    "    for i in range(pts1.shape[1]):\n",
    "        A = np.array([pts1_homo[1,i]*P1[2,:] - P1[1,:],\n",
    "            P1[0,:] - pts1_homo[0,i]*P1[2,:],\n",
    "            pts2_homo[1,i]*P2[2,:] - P2[1,:],\n",
    "            P2[0,:] - pts2_homo[0,i]*P2[2,:]])\n",
    "        ATA = A.T @ A\n",
    "        _, _, Vt = np.linalg.svd(ATA)\n",
    "        pts3d[:, i] = Vt[-1, :3]/Vt[-1, -1]\n",
    "    \n",
    "    return pts3d\n",
    "\n",
    "def linear_triangulation2(P1, P2, pts1, pts2):\n",
    "    # First, set all points to homogeneous\n",
    "    pts1_homo = np.vstack((pts1, np.ones(pts1.shape[1])))\n",
    "    pts2_homo = np.vstack((pts2, np.ones(pts2.shape[1])))\n",
    "\n",
    "    # Solve using svd\n",
    "    pts3d = np.zeros((3, pts1.shape[1]))\n",
    "    for i in range(pts1.shape[1]):\n",
    "        A = np.array([pts1_homo[1,i]*P1[2,:] - P1[1,:],\n",
    "                      P1[0,:] - pts1_homo[0,i]*P1[2,:],\n",
    "                      pts2_homo[1,i]*P2[2,:] - P2[1,:],\n",
    "                      P2[0,:] - pts2_homo[0,i]*P2[2,:]])\n",
    "        ATA = A.T @ A\n",
    "        _, _, Vt = np.linalg.svd(ATA)\n",
    "        pts3d[:, i] = Vt[-1, :3]/Vt[-1, -1]\n",
    "    \n",
    "    return pts3d"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "K1, K2 = K, K\n",
    "RT2 = RT2s[0]\n",
    "\n",
    "pts3d = np.array([linear_triangulation(K, RT1, K, RT2, pts1, pts2) for RT2 in RT2s])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "source": [
    "def reprojection(P1, P2, pts3d):\n",
    "    pts3d_homo = np.vstack((pts3d, np.ones(pts3d.shape[1])))\n",
    "    pts2d1_homo = np.dot(P1, pts3d_homo)\n",
    "    pts2d2_homo = np.dot(P2, pts3d_homo)\n",
    "    return pts2d1_homo/pts2d1_homo[-1], pts2d2_homo/pts2d2_homo[-1]\n",
    "\n",
    "def double_disambiguation(K1, RT1, K2, RT2s, pts1, pts2, pts3d):\n",
    "    max_positive_z = 0\n",
    "    min_error = np.finfo('float').max\n",
    "    best_RT = None\n",
    "    best_pts3d = None\n",
    "    P1 = K1 @ RT1\n",
    "\n",
    "    pts1_homo = np.vstack((pts1, np.ones(pts1.shape[1])))\n",
    "    pts2_homo = np.vstack((pts2, np.ones(pts2.shape[1])))\n",
    "\n",
    "    for i in range(RT2s.shape[0]):\n",
    "        P2 = K2 @ RT2s[i]\n",
    "        num_positive_z = np.sum(pts3d[i][2, :] > 0)\n",
    "        re1_pts2, re2_pts2 = reprojection(P1, P2, pts3d[i])\n",
    "\n",
    "        err1 = np.sum(np.square(re1_pts2 - pts1_homo))\n",
    "        err2 = np.sum(np.square(re2_pts2 - pts2_homo))\n",
    "\n",
    "        err = err1 + err2\n",
    "\n",
    "        if num_positive_z >= max_positive_z and err < min_error:\n",
    "            max_positive_z = num_positive_z\n",
    "            min_error = err\n",
    "            best_RT = RT2s[i]\n",
    "            best_pts3d = pts3d[i]\n",
    "    \n",
    "    return best_RT, best_pts3d"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "source": [
    "RT2, pts_cloud = double_disambiguation(K, RT1, K, RT2s, pts1, pts2, pts3d)\n",
    "pts_cloud = pts_cloud[:, pts_cloud[2, :] > 0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "source": [
    "# Visualize 3D points\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming points_3D is your array of 3D points\n",
    "x = pts_cloud[0]\n",
    "y = pts_cloud[1]\n",
    "z = pts_cloud[2]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,\n",
    "                                   mode='markers',\n",
    "                                   marker=dict(size=2, color=z, colorscale='Viridis'))])\n",
    "\n",
    "fig.update_layout(scene=dict(xaxis_title='X',\n",
    "                             yaxis_title='Y',\n",
    "                             zaxis_title='Z'))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "source": [
    "# Initialize general Projection matrix list, RTs list and 3d points list\n",
    "P_list = np.ndarray((images.shape[0], 3, 4))\n",
    "P_done = np.full((images.shape[0], 1), False)\n",
    "P_list[0], P_done[0] = K @ RT1, True\n",
    "P_list[1], P_done[1] = K @ RT2, True"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Increment SfM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find matches between image 3 and images 1 and 2\n",
    "\n",
    "First we'll start one by one and then we'll do a more general form of this for N images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "# Extract features\n",
    "img3 = images[2]\n",
    "kp, des = feature_extraction_set([img1, img2, img3])\n",
    "\n",
    "# Match features\n",
    "matches = feature_matching_set(kp, des)\n",
    "\n",
    "common_matches1 = [m1 for m1 in matches[(0, 2)] for m2 in matches[(1, 2)] if m1.trainIdx == m2.trainIdx]\n",
    "common_matches2 = [m2 for m1 in matches[(0, 2)] for m2 in matches[(1, 2)] if m1.trainIdx == m2.trainIdx]"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projection Matrix\n",
    "\n",
    "Next, we triangulate the position of the common points to get the 3D point corresponding to each 2D point we can. That way, we can use the DLT to get the projection matrix of the third image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "pts1 = np.transpose([kp[0][m.queryIdx].pt for m in common_matches1])\n",
    "pts2 = np.transpose([kp[1][m.queryIdx].pt for m in common_matches2])\n",
    "pts3 = np.transpose([kp[2][m.trainIdx].pt for m in common_matches1])\n",
    "\n",
    "pts3d = linear_triangulation2(P_list[-2], P_list[-1], pts1, pts2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "def calculate_projection_matrix(K, pts3d, pts2d):\n",
    "    _, rod, T, _ = cv2.solvePnPRansac(pts3d.T, pts2d.T, K, None)#, flags=cv2.SOLVEPNP_P3P)\n",
    "    R = cv2.Rodrigues(rod)[0]\n",
    "    if np.linalg.det(R) < 0:\n",
    "        R = R * -1\n",
    "    P = K @ np.hstack((R, T))\n",
    "    return P"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "P_list.append(calculate_projection_matrix(pts3d, pts3))"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triangulation\n",
    "\n",
    "Finally, we triangulate the position of each matching points to get more 3D points we'll add to the pts_cloud and then we visualize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "pts1 = np.transpose([kp[0][m.queryIdx].pt for m in matches[(0,2)]])\n",
    "pts2 = np.transpose([kp[2][m.trainIdx].pt for m in matches[(0,2)]])\n",
    "\n",
    "pts3d = linear_triangulation2(P_list[0], P_list[2], pts1, pts2)\n",
    "pts_cloud = np.hstack((pts_cloud, pts3d))\n",
    "\n",
    "pts1 = np.transpose([kp[1][m.queryIdx].pt for m in matches[(1,2)]])\n",
    "pts2 = np.transpose([kp[2][m.trainIdx].pt for m in matches[(1,2)]])\n",
    "\n",
    "pts3d = linear_triangulation2(P_list[1], P_list[2], pts1, pts2)\n",
    "pts3d = pts3d[:, pts3d[2, :] > 0]\n",
    "pts_cloud = np.hstack((pts_cloud, pts3d))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Visualize 3D points\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming points_3D is your array of 3D points\n",
    "x = pts_cloud[0]\n",
    "y = pts_cloud[1]\n",
    "z = pts_cloud[2]\n",
    "\n",
    "fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,\n",
    "                                   mode='markers',\n",
    "                                   marker=dict(size=2, color=z, colorscale='Viridis'))])\n",
    "\n",
    "fig.update_layout(scene=dict(xaxis_title='X',\n",
    "                             yaxis_title='Y',\n",
    "                             zaxis_title='Z'))\n",
    "\n",
    "fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Generalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, with only one sample we can see what the pattern would look like. So we start the main loop to get all the 3D points for the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "def plot_model(pts_cloud):\n",
    "    # Assuming points_3D is your array of 3D points\n",
    "    x = pts_cloud[0]\n",
    "    y = pts_cloud[1]\n",
    "    z = pts_cloud[2]\n",
    "\n",
    "    fig = go.Figure(data=[go.Scatter3d(x=x, y=y, z=z,\n",
    "                                    mode='markers',\n",
    "                                    marker=dict(size=2, color=z, colorscale='Viridis'))])\n",
    "\n",
    "    fig.update_layout(scene=dict(xaxis_title='X',\n",
    "                                yaxis_title='Y',\n",
    "                                zaxis_title='Z'))\n",
    "\n",
    "    fig.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "def two_images_sfm(im1, im2, K):\n",
    "    # Feature extraction\n",
    "    kp, des = feature_extraction_set([im1, im2])\n",
    "\n",
    "    # Feature matching\n",
    "    matches = feature_matching_set(kp, des)\n",
    "\n",
    "    # Fundamental matrix\n",
    "    pts1 = np.transpose([kp[0][m.queryIdx].pt for m in matches[(0,1)]])\n",
    "    pts2 = np.transpose([kp[1][m.trainIdx].pt for m in matches[(0,1)]])\n",
    "\n",
    "    F = eight_point_algorithm(pts1, pts2)\n",
    "\n",
    "    # Essential matrix\n",
    "    E = essential_from_fundamental(K, F, K) # In this case, the same intrinsic values apply to all images\n",
    "\n",
    "    # Get camera extrinsics from Essential matrix\n",
    "    RT2s = pose_from_essential(E)\n",
    "\n",
    "    # Define RT for camera 1 (center at world origin and matching orientation)\n",
    "    RT1 = np.hstack((np.eye(3), np.zeros((3, 1))))\n",
    "    \n",
    "    RT2 = RT2s[0]\n",
    "\n",
    "    pts3d = np.array([linear_triangulation(K, RT1, K, RT2, pts1, pts2) for RT2 in RT2s])\n",
    "\n",
    "    RT2, pts_cloud = double_disambiguation(K, RT1, K, RT2s, pts1, pts2, pts3d)\n",
    "\n",
    "    # Initialize general Projection matrix list, RTs list and 3d points list\n",
    "\n",
    "    return K @ RT1, K @ RT2, pts_cloud"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "# Load all images\n",
    "images = np.array(load_images_from_folder('dinos'))\n",
    "\n",
    "images_tmp1 = images[:16]\n",
    "images_tmp2 = images[17:]\n",
    "\n",
    "images = np.concatenate((images_tmp1, images_tmp2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# K = np.loadtxt('intrinsic_matrix.txt', dtype=float)\n",
    "\n",
    "height, width = images.shape[1:3]\n",
    "K = np.array([  # for dino\n",
    "    [2360, 0, width / 2],\n",
    "    [0, 2360, height / 2],\n",
    "    [0, 0, 1]])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "source": [
    "kp, des = feature_extraction_set(images)\n",
    "matches = feature_matching_set(kp, des)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "source": [
    "image_count = 0\n",
    "pts_cloud = []\n",
    "P_list = []\n",
    "\n",
    "while image_count < images.shape[0]:\n",
    "    P1, P2, pts3d = two_images_sfm(images[image_count], images[image_count+1], K)\n",
    "    pts_cloud.append(pts3d)\n",
    "    image_count += 2\n",
    "    P_list.append([P1, P2])\n",
    "\n",
    "    for i in range(image_count, images.shape[0]):\n",
    "        idx1, idx2, idx3 = i-2, i-1, i\n",
    "        image_count += 1\n",
    "        \n",
    "        # Get Common matches of 3 images\n",
    "        common_matches1 = [m1 for m1 in matches[(idx1, idx3)] for m2 in matches[(idx2, idx3)] if m1.trainIdx == m2.trainIdx]\n",
    "        common_matches2 = [m2 for m1 in matches[(idx1, idx3)] for m2 in matches[(idx2, idx3)] if m1.trainIdx == m2.trainIdx]\n",
    "        \n",
    "        if len(common_matches1) < 4:\n",
    "            print(i)\n",
    "            print('STOP')\n",
    "            break\n",
    "\n",
    "        # Triangulate common points\n",
    "        common_pts1 = np.transpose([kp[idx1][m.queryIdx].pt for m in common_matches1])\n",
    "        common_pts2 = np.transpose([kp[idx2][m.queryIdx].pt for m in common_matches2])\n",
    "        common_pts3 = np.transpose([kp[idx3][m.trainIdx].pt for m in common_matches1])\n",
    "\n",
    "        common_pts3d = linear_triangulation2(P_list[-1][-2], P_list[-1][-1], common_pts1, common_pts2)\n",
    "\n",
    "        # Get Projection matrix\n",
    "        P = calculate_projection_matrix(K, common_pts3d, common_pts3)\n",
    "\n",
    "        P_list[-1].append(P)\n",
    "        \n",
    "        # Triangulate\n",
    "        pts1_list = [np.transpose([kp[j][m.queryIdx].pt for m in matches[(j,idx3)]]) for j in range(idx1, idx3)]\n",
    "        pts2_list = [np.transpose([kp[idx3][m.trainIdx].pt for m in matches[(j,idx3)]]) for j in range(idx1, idx3)]\n",
    "\n",
    "        for j in range(2):\n",
    "            pts3d = linear_triangulation2(P_list[-1][-3+j], P_list[-1][-1], pts1_list[j], pts2_list[j])\n",
    "            pts_cloud[-1] = np.hstack((pts_cloud[-1], pts3d))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "source": [
    "RT = np.linalg.inv(K) @ P_list[1][-1]\n",
    "transf = np.vstack((RT, [0, 0, 0, 1]))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "source": [
    "# stop = False\n",
    "# for i in range(2, images.shape[0]):\n",
    "#     # Extract features\n",
    "#     window = min(2, i)\n",
    "\n",
    "#     # Get Common matches of 3 images\n",
    "#     batrakin = 0\n",
    "#     common_matches1 = [m1 for m1 in matches[(i-2-batrakin, i)] for m2 in matches[(i-1-batrakin, i)] if m1.trainIdx == m2.trainIdx]\n",
    "#     common_matches2 = [m2 for m1 in matches[(i-2-batrakin, i)] for m2 in matches[(i-1-batrakin, i)] if m1.trainIdx == m2.trainIdx]\n",
    "    \n",
    "#     if len(common_matches1) < 4:\n",
    "#         print('STOP')\n",
    "#         break\n",
    "\n",
    "#     # Triangulate common points\n",
    "#     common_pts1 = np.transpose([kp[i-2-batrakin][m.queryIdx].pt for m in common_matches1])\n",
    "#     common_pts2 = np.transpose([kp[i-1-batrakin][m.queryIdx].pt for m in common_matches2])\n",
    "#     common_pts3 = np.transpose([kp[i][m.trainIdx].pt for m in common_matches1])\n",
    "\n",
    "#     common_pts3d = linear_triangulation2(P_list[i-2-batrakin], P_list[i-1-batrakin], common_pts1, common_pts2)\n",
    "\n",
    "#     # Get Projection matrix\n",
    "#     P = calculate_projection_matrix(K, common_pts3d, common_pts3)\n",
    "\n",
    "#     P_list[i], P_done[i] = P, True\n",
    "    \n",
    "#     # Triangulate\n",
    "#     pts1_list = [np.transpose([kp[j][m.queryIdx].pt for m in matches[(j,i)]]) for j in range(i-window, i)]\n",
    "#     pts2_list = [np.transpose([kp[i][m.trainIdx].pt for m in matches[(j,i)]]) for j in range(i-window, i)]\n",
    "\n",
    "#     for j in range(window):\n",
    "#         pts3d = linear_triangulation2(P_list[i-window+j], P_list[i], pts1_list[j], pts2_list[j])\n",
    "#         # model_condition = ((pts3d[0, :] >= intervals_3d[0,0]) & (pts3d[0,:] <= intervals_3d[0,1]) &\n",
    "#         #                    (pts3d[1, :] >= intervals_3d[1,0]) & (pts3d[1,:] <= intervals_3d[1,1]) &\n",
    "#         #                    (pts3d[2, :] >= intervals_3d[2,0]) & (pts3d[2,:] <= intervals_3d[2,1]))\n",
    "#         # pts3d = pts3d[:, model_condition]\n",
    "#         pts_cloud = np.hstack((pts_cloud, pts3d))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "source": [
    "for pts in pts_cloud:\n",
    "    plot_model(pts)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "source": [
    "pts_cloud[0] = np.linalg.inv(RT[:3, :3]) @ pts_cloud[0]\n",
    "#pts_cloud[0] = np.array([pt + RT[:3, -1] for pt in pts_cloud[0, :]]).T\n",
    "\n",
    "pts_cloud = np.hstack(pts_cloud)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "source": [
    "plot_model(pts_cloud)"
   ],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
